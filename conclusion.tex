\section{Future Work}
The study of social influence in the CRC has several interesting directions for future work.
We would like to run user studies comparing a control group that did not see the median grade, 
a treatment group who saw an incorrect median grade, and testing for the effect of an increased delay in revealing the median grade.
Such a study would shed light on the complexities and nuances of social influence in rating systems.
 
We want to extend our work to quantify the role of social influence in textual data. 
The California Report Card collects textual suggestions from participants in addition to the quantiative assesment results. 
Participants are encouraged to read the responses of others before leaving a suggestion of their own.
We suspect that this may lead to a bias in the topics discussed by participants, and we would like to explore how similar non-parametric models can be extended to textual data.
We hypothesize that unlike social herding, participants may try to differentiate their ideas from ones they have already ready.

A compelling statistical direction is to attempt to parameterize our model.
We will explore whether we can model the grades as a mixture of binomial distributions (a discrete analog of a mixture of Gaussians), and try to derive optimal tests and models for this data.
We believe that proper parametrization, that accurate models multimodality and discreteness, should lead to increased statistical power and better fitting models.

\section{Conclusion}
Revealing aggregate statistics is a common feature of many rating systems.
We worked with a new platform, the California Report Card, which revealed the median rating after participants left their own rating.
Participants has the opportunity to change their ratings after seeing the median for the population.
We proposed non-parametric hypothesis tests based on the Wilcoxon rank-sum test to determine whether or not revealing the median encouraged participants to \emph{herd} towards the median grade.
Comparing the data to a randomized reference suvery, we found that changed grades were statistically significantly closer to the median.
We modeled these changes with polynomial function, whose degree we chose optimally using the Bayes Information Criterion. 
We found that this relationship was linear in four out of the six issues suggesting homogeneity in biasing for positive and negative differences with the median.
For remaining two issues, we found that the positive bias was more significant than the negative one.

The motivation for a non-parametric approach was to make as few modeling assumptions about the distribution of ratings.
Recommender and rating systems use a variety of different input techniques eg. sliders, stars, drop-down menus etc., all with their own biases and data distributions.
In principle, the methods we proposed can be applied to test and model biases for a wide variety input mechanisms.

The purpose of this paper was to test, model, and predict the effects of social influence bias in rating systems.
While we found that the bias was significant, this result should not be interpreted as an argument against displaying aggregate statistics while collecting ratings.
In fact, we are encouraged by our results since they suggest that we can model and predict the influence process quite well.
Our experiments indicate that that effects of social influence in rating data is nuanced and subtle varying question to question with non-linearities.
Modeling this phenomenon can help improve recommender systems that derive their recommendations from ratings that were collected with such biases.
