\section{Parameter Estimation For Grade Change Model}
\label{changemod}
In the previous sections, we proposed a technique to test the significance of the regression towards the median.
In this section, we build a model to describe the relationship between the variables in the 3-tuple ($g_i[j]$, $m[j]$, $g_f[j]$).
We also contrasted two different parameters of interest: correlation and absolute deviation.
In this section, we will further build on this to estimate two quantities: a functional relationship between $m[j] - g_i[j]$ and $g_f[j] - g_i[j]$, and a quantification of how much more concentrated the changed grades are $\Delta$.
The functional relationship, related to the correlation, will tell us how to predict a final grade given an observed median.
The $\Delta$ parameter will tell us how much more tightly grouped around the median the final grades are.

\subsection{Modeling Heterogenous Changes}
Previous work, suggests that Social Influence bias is not homogenous; that is a negative influence is different in magnitude than a positive influence \cite{???}.
This means that we cannot assume that the relationship between $m[j] - g_i[j]$ and $g_f[j] - g_i[j]$ is linear.

Similar to the previous section where we applied non-parametric tests, we propose a information theoretic model search that allows flexible parameter selection without making strong assumptions about the nature of the relationship.
Let $f\in \mathcal{P}^k$ be a polynomial of degree $k$.
The square loss of $f$, is the error in predicting $g_f[j] - g_i[j]$ from $f(m[j] - g_i[j])$:
\begin{equation}
\mathcal{L}(X_c;f,k) = \sum_j ((g_f[j] - g_i[j]) - f(m[j] - g_i[j]))^2 
\end{equation}
For a given $k$, the best-fit polynomial minimizes this square-loss:
\begin{equation}
f^*_k =\arg \min_f \mathcal{L}(X_c;f,k)
\end{equation}
To search over the space of polynomial models, we apply a well-studied technique called the Bayesian Information Criterion (BIC) \cite{???}.
This penalty can be interpreted as bias towards lower degree models, in other words, an Occam's Razor prior belief. 
This we reformulate the optimization problem in the following way to incorporate the BIC:
\begin{equation}
\arg \min_{f,k} |X_c|\log(\mathcal{L}(X_c;f,k)) + k\log(|X_c|)
\end{equation}
The resulting optimal polynomal will tell how the regression affects varies as a function of $m[j] - g_i[j]$ while controlling for over-fitting to our data.
This optimization problem is non-convex so we incrementally try polynomials of degree 1,2,3.. etc. until we reach a local minimum.

\subsection{Concentration of Grades}
We can further estimate how much more concentrated changed grades are around the observed median.
Recall in Section \ref{ht}, we tested the significance of the absolute deviations using a Wilcoxon test statistic.
The Wilcoxon statistic can be inverted to estimate a most likely \emph{shift parameter}, that a constant shift $\Delta$ in the distribution of absolute deviations $X_c$ that maximally aligns them with $X_n$ (ie. $X_c + \Delta$ is most supported by the null hypothesis). 
Since $X_c$ is a set of absoulte deviations, $\Delta$ tells us how much more concentrated $X_c$ is than $X_n$ around the observed medians.
This parameter is relevant to the design of recommendation algorithms use proximity (eg. clustering or nearest neighbors).

We refer to \cite{???} on the derivation of $\Delta$ and its confidence interval:
\begin{equation}
D = \{x_n[j] - x_c[i]\} \text{ } \forall i,j \in X_n, X_c
\end{equation}
\begin{equation}
\Delta = median(D)
\end{equation}




