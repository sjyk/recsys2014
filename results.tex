\section{Results}
We evaluated our models on data collected from the California Report Card between January 18th to April 20th.
We administered our reference survey through SurveyMonkey between March 8th and March 14th.
We consider a set of 1575 total participants from the CRC and a sample of 611 SurveyMonkey participants whose grading activity was as follows:\\[1\baselineskip]

{\centering\scriptsize
\begin{tabular}[!ht]{ l | r | r | r | l }
Issue & No Change & Change & Skip & Median \\
\hline
\hline
  \multicolumn{5}{l}{\textbf{CRC}}\\
  \hline
  Obamacare & 749 & 223 & 593 & B \\
  \hline
  K12 & 849 & 172 & 544 & C+ \\
  \hline
  College & 923 & 139 & 503 & C-\\
  \hline
  Immigration & 693 & 105 & 767 & C \\
  \hline
  Marijuana & 881 & 118 & 566 & C \\
  \hline
  Marriage Rights & 929 & 105 & 531 & B+\\
\hline
\hline
\multicolumn{5}{l}{\textbf{Reference}}\\
\hline
  Obamacare & 498 & - & 113 & B \\
  \hline
  K12 & 561 & - & 50 & C \\
  \hline
  College & 573 & - & 38 & C-\\
  \hline
  Immigration & 375 & - & 236 & C+ \\
  \hline
  Marijuana & 498 & - & 113 & C \\
  \hline
  Marriage Rights & 554 & - & 57 & B+
\end{tabular}\\[1\baselineskip]
}

For any given issue, between 10\% and 20\% of those who assigned grades registered a grade change.
In all, 556 out of the 1575 CRC participants changed their grades at least once (Figure \ref{change-1}).
We also found that the aggregate results of the reference survey matched the CRC quite well.
On only two issues (K12 and Immigration), we found a observed differences which were both less than a letter grade (+ or -).
In our evaluation of these two surveys, we will use the unit \emph{full letter grades}.
For example, one full letter grade corresponds to the difference between an A grade and a B grade. 
A difference of a + or - is represented as $\frac{1}{3}$ eg. B to B+ or B+ to A-. 

\begin{figure}[h]
\hspace*{-2em}
    \includegraphics[scale=.25]{../plots/change-1.eps}
    \hspace*{-2em}
    \includegraphics[scale=.25]{../plots/change-2.eps}
      \caption{The majority of participants did not change grades. 35\% of participants changed their grade on at least one issue, the majority of which (62\%) changed only a single issue. A majority of the changes were towards the median grade.}
      \label{change-1}
\end{figure}

\subsection{Moving Towards the Median}
Using the non-parametric test proposed in Section \ref{ht}, we tested the hypothesis of whether grade changes led to significantly more concentration around the median grade.
In our first experiment (Figure \ref{mdev-1}), we tested the absolute deviations of only the CRC participants.
We compared the group of participants that did not change their grades to the group that changed their grades.
We found that while there were no statistically significant differences between the initial grades of the two groups, the final grades of the group that changed were statistically significantly more concentrated than both their own initial grades and the grades of the no change group.
For the set of participants who changed their grades $P_c$ and those who did not $P_n$:
\begin{figure}[h]
\hspace{-2em}
    \includegraphics[scale=0.27]{../plots/abs-deviations-1.eps}
      \caption{For those participants that changed their grades, final grades were significantly more concentrated around the median grade than their initial grades. In addition, these grades are more concentrated than the grades for those who didn't change.}
      \label{mdev-1}
\end{figure}

{\centering
\scriptsize
\begin{tabular}[!ht] { r | r | r }
\label{dev-2}
  Issue & p-val($P_c$ vs. $P_n$) & p-val($i$ vs. $f$) \\
  \hline
  \hline
  Obamacare &  0.0286 & 0.0161 \\
  \hline
  K12 & 2.1314e-06 &  0.0086 \\
  \hline
  College & 1.3033e-04 & 0.0415 \\
  \hline
  Immigration & 7.3456e-07 &4.4170e-05\\
  \hline
  Marijuana & 2.7549e-10 & 4.2560e-05\\
  \hline
  Marriage Rights & 3.5946e-06 & 2.4644e-10 \\
\end{tabular}\\[1\baselineskip]
}

These results are consistent with social influence bias.
When participants change their grades, they are more likely to concentrate around the median.
What is particularly surprising is that the two groups of participants $P_n$ and $P_c$ are very similar in terms of initial grades, and the data suggests that the bias is not correlated with more or less concentrated initial grades.

In our second experiment (Figure \ref{mdev-2}), we apply the same testing procedure to compare the grades from the CRC to to those in the reference survey.
We compare absolute deviations of the group of participants who changed their grades in the CRC against participants from the reference survey.
\begin{figure}[h]
\hspace{-2.6em}
    \includegraphics[scale=0.27]{../plots/bias-2.eps}
      \caption{We found that final grades were significantly more concentrated in the CRC compared to grades in the reference survey. Similar to Figure \ref{mdev-1}, we found that there was no statistically significant difference between the reference survey and the initial grades.}
      \label{mdev-2}
\end{figure}

{\centering
\scriptsize
\begin{tabular}[!ht] { r | r | r }
\label{ref-1}
  Issue & p-val($R$ vs. $i$) & p-val($R$ vs. $f$) \\
  \hline
  \hline
  Obamacare &  0.5386 & 0.0015 \\
  \hline
  K12 & 0.8283 & 0.0097 \\
  \hline
  College & 0.1452 & 0.0091 \\
  \hline
  Immigration & 0.3765 & 1.1787e-04\\
  \hline
  Marijuana & 0.7288 & 9.3111e-06\\
  \hline
  Marriage Rights & 0.2478 & 0.0161 \\
\end{tabular}\\[1\baselineskip]
}

The results of our two experiments suggest that the CRC rating data is affected by social herding.
We not only found that participants' changed grades were statistically significantly more likely to concentrate around the median, they were also more likely in comparison to the reference survey.
While correlation does not imply causation, we argue that this evidence rejects the null hypothes and is most consistent with \textbf{Hypothesis 1}.
As the CRC was not a randomized survey, there are possibly confounding covariates eg. participants who changed their grades were more likely to leave tightly concentrated grades in the first place.
However, our comparison with the reference survey, and discovery that initial grades were largely consistent with the reference survey and with those that didn't change their grades, suggest that these confounding covariates are not very significant.
These results are encouraging and we hope to run a randomized participant study to confirm the causal relationship between revealing the median and concentrated grades.

\begin{figure}[ht!]
\centering
  \hspace{-3em}
    \includegraphics[scale=0.25]{../plots/shift-parameter.eps}
      \caption{We calculate the parameter $\Delta$ which is the most likely amount that our observations deviate from the null hypothesis. This can be interpreted as how much more are grades in our change group concentrated around the median.}
      \label{shift-1}
\end{figure}
\subsection{Estimating the Social Influence Bias}
We tested the hypotheses and conclude significant additional concentration of grades around the median grade.
In Section \ref{ht}, we described how we could use the results of the hypothesis test to estimate the $\Delta$ parameter, which quantifies how different the hypothesis is from the null distribution.
In Figure \ref{shift-1}, we show the parameter estimates for each of the issues.
As before, the units of the plot are in terms of letter grades.
For the issues about Marriage Rights, we find that parameter is 2/3 of a letter grade.
This means that the set of absolute deviations for the change group $X_c$ was on average 2/3 of a letter grade smaller.
For the other issues, the parameter was smaller indicating less of an effect of social herding.
The parameter was the smallest for the first issue (Obamacare), and we conjecture that for this issue many participants were still learning how to use the slider interface; leading to random grade changes.
For this issue, we see that it had the most number of grade changes as well.

This parameter is very relevant to the design of both recommender systems and predictive models.
Consider the problem of trying to predict a participant's next grade.
The simplest model we could design is a model where we always select the median grade.
Such a model is often used as a baseline for comparison in recommender systems.
What we may interpret as low prediction error may in fact be the effects of social herding around the median, and if we were to apply this model asking the same questions but in a system without the herding effects; we may find that the same model performs poorly.
A median prediction is a naive model, but this problem affects many recommendation algorithms since they often rely on proximity metrics such as clustering, k-nearest neighbors, and some kernel machine learning methods.
\subsection{Sequential Evolution of Social Influence}
Using the model proposed in Section \ref{path}, we calculated the test statistics for both the CRC and the Reference Survey.
Recall, that this model calculates the change in disagreement with the median; that is, how much did a participant disagree with the median on past responses compared 
to the current value.
A comparatively larger value in the CRC suggests repeated feedback of the median grade encourages future responses to be more moderate.
Figure \ref{path-1} illustrates the results of the experiment for the five last issues (K12, College, Immigration, Marijuana, Marriage Rights), and using the mean disagreement on the previous issues.
The experimental results were not as significant as the other experiments and we found that only two issues Immigration (p=0.0037) and Marijuana (p=0.0287) showed an order dependence that was significantly greater than the reference survey.

\begin{figure}[h]
	\hspace{-2em}
    \includegraphics[scale=0.27]{../plots/path-dependence.eps}
      \caption{We measure the mean change in disagreement for each issue. The change in disagreement is defined as the average deviation with median on previous issues minus the deviation with the median on the current issue. We hypothesized that the CRC would have comparatively greater (or less negative) values than the reference survey.}
      \label{path-1}
\end{figure}
In summary, we were not able to reject the null hypothesis, but we did find some indication with two statistically significant results that it plays an important role.
This underscores the subtlety social influence bias which is question and context dependent.

\begin{figure*}[ht!]
\hspace{-7em}
    \includegraphics[scale=0.36]{../plots/BIC-optimization.eps}
      \caption{For the participants that changed their grades, we plot the difference between their grade and the median (X-axis), and their changed grade (Y-axis). We overlay the optimal polynomial model to represent the relationship $f(x) = y$. Below each plot, is the BIC objective function showing how we picked an optimal degree of polynomial.}
      \label{opt-1}
\end{figure*}
\subsection{Predicting Grade Changes}
We train the polynomial model proposed in Section \ref{changemod}, and the results are shown in Figure \ref{opt-1}.
Our model search and optimization through the BIC discovered that for four out of the six issues, K12, College, Immigration, and Marijuana, the model was linear.
This suggests homogeneity in positive and negative social influence effects for these issues.
However, for Obamacare and Marriage Rights, we found that the relationship was quadratic.
Interestingly enough, over the domain of changes, the learned quadratic was ``almost" linear, however it modeled heterogeneity in positive and negative influence effects.
Participants who initially graded the state higher than the median had a more significant tendency to change downwards, in comparison to the upward tendency of those who graded less than the median.

These models further illustrate the subtlety of social influence bias. 
In Muchnik et al. they observed a biasing tendency where both positive and negative influence led to increased upward bias (due to herding and correcting tendencies respectively).
Compared to those results, our results are quite different as we observe movement towards the median for both positive and negative influences.
One possible explanation is that since our mechanism for social influence is revealing the true grade, not modifying the grade (thus showing an ``incorrect" value), the correcting tendency is not significant.

We also tested our model in terms of RMSE prediction error (Figure \ref{poly-1}). 
We held out 20\% of the pairs of ratings (observed change, actual) and tested the prediction error on this held out set.
We found that our model on average, over all issues, predicted final grade within $0.5150$ letter grades.
Note that 1/3 of a letter grade corresponds to the difference of just a + or - grade.

In a second experiment (Figure \ref{poly-1}), we re-calculated the shift-parameter $\Delta$ after applying the inverse model defined in Section \ref{changemod}.
We found that there was on average a 76.3\% reduction in $\Delta$ which quantifies the concentration around the median. 
This suggests that our model can not only predict final grades but also infer initial grades with high accuracy.

\begin{figure}[h]
\hspace*{-2em}
    \includegraphics[scale=0.25]{../plots/prediction-error.eps}
    \hspace*{-2em}
    \includegraphics[scale=0.24]{../plots/shit-2.eps}
      \caption{We measured the RMSE prediction error of the polynomial model. We found that we could predict changes in all of the issues with less than 2/3 of a letter grade RMSE error. We applied this model to correct for the social influence bias and found that, on average, we could reduce the effects by 76.3\%}
      \label{poly-1}
\end{figure}




